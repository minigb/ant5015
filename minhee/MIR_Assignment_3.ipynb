{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05bbdb28",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "- In this assignment, you will implement a melody-language model using Essen Folksong dataset\n",
    "- You have to submit your code in **TWO** formats:\n",
    "    - Completed Notebook with `.ipynb`\n",
    "    - A `MIR_Assignment_3_{your_student_number}.py` file that includes **ALL functions and classes you have completed**\n",
    "        - Do not include any other code except function and class\n",
    "        - Your result will be scored by an evaluation code that import this `MIR_Assignment_3_{your_student_number}.py` file\n",
    "        - So be careful not to use any global variable inside the function\n",
    "- You have to submit a report (optional) and **three** generation results of your favorite in wav files\n",
    "    - The report is optional. If you have tried other architecture for MelodyLanguage Model, you can describe the result.\n",
    "\n",
    "\n",
    "- Caution: The `assert` lines are designed to check whether basic requirements are satisfied. Even though you passed all the assert cases, it doesn't guarantee that your implementation is fully correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f65852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download Assignment_3.py\n",
    "# !wget https://raw.githubusercontent.com/jdasam/ant5015/main/MIR_Assignment_3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d255bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24cd5c",
   "metadata": {},
   "source": [
    "## 0. Prepare (Install and import library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e670ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip downloading as the Bravura font is found.\n",
      "Skip downloading as the MuseScore General soundfont is found.\n"
     ]
    }
   ],
   "source": [
    "# !pip install muspy\n",
    "import muspy\n",
    "\n",
    "muspy.download_bravura_font()\n",
    "'''\n",
    "You may have to install fluidsynth.\n",
    "In Colab, you can install by followign code\n",
    "\n",
    "!sudo apt-get install fluidsynth\n",
    "'''\n",
    "muspy.download_musescore_soundfont()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ac9b3ba",
   "metadata": {},
   "source": [
    "## Problem 1: Understanding and Implementing RNN (15 pts)\n",
    "- Recurrent neural network is a typical choice for handling sequential data with a neural network\n",
    "- In this problem, you have to implement a Vanilla RNN\n",
    "    - For each time step $t$, RNN takes two inputs\n",
    "        - $x_t$, which is an input vector of time step $t$\n",
    "        - $h_{t-1}$, which is an hidden state of previous time step, $t-1$\n",
    "            - $h_{t-1}$ is also the output of RNN for previous time step $t-1$\n",
    "    - For given $x_t$ and $h_{t-1}$, RNN returns $h_t$\n",
    "        - $h_t = \\tanh(\\textbf{W}x_t + \\textbf{U}h_{t-1} + b)$ \n",
    "            - $\\textbf{W}$ and $\\textbf{U}$ is a trainable weight matrix of RNN\n",
    "            - $\\textbf{W} \\in \\mathbb{R}^{d \\times h}$, and $\\textbf{U} \\in \\mathbb{R}^{h \\times h}$, where $d$ is number of input dimension and $h$ is number of hidden state dimension\n",
    "                - This means that $\\textbf{W}$ is a matrix with real numbers and size of $\\text{num\\_input\\_dim} \\times \\text{num\\_hidden\\_dim}$  \n",
    "                - and $\\textbf{U}$ is a matrix with real numbers and size of $\\text{num\\_hidden\\_dim}\\times \\text{num\\_hidden\\_dim}$  \n",
    "            \n",
    " - The output of fully connected layer (`nn.Linear`) for a given input vector $x$ is as below:\n",
    "     - $\\text{output} = \\textbf{W}x+b$\n",
    "     - Where $\\textbf{W}$ is a weight matrix and $b$ is a bias vector\n",
    "     - Both $\\textbf{W}$ and $b$ are trainable parameters\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050d35e",
   "metadata": {},
   "source": [
    "### Problem 1.1: Calculating Forward Propagation of RNN\n",
    "- Based on the example above, implement the forward propagation of uni-directional, single layer vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3564e530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_weight_for_hidden_to_hidden: \n",
      " tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920],\n",
      "        [-0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        [ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959],\n",
      "        [ 0.5667,  0.7935,  0.4397,  0.1124,  0.6408,  0.4412],\n",
      "        [-0.2159, -0.7425,  0.5627,  0.2596,  0.5229,  2.3022],\n",
      "        [-1.4689, -1.5867,  1.2032,  0.0845, -1.2001, -0.0048]])\n",
      "example_weight_for_input_to_hidden: \n",
      " tensor([[-0.2303, -0.3918, -0.4731],\n",
      "        [ 0.3356,  1.5091,  2.0820],\n",
      "        [ 1.7067,  2.3804, -1.1256],\n",
      "        [-0.3170, -0.1407,  0.8058],\n",
      "        [ 0.3276, -0.7607, -1.5991],\n",
      "        [ 0.0185, -0.7504,  0.1854]])\n",
      "example_bias: \n",
      " tensor([-0.6776,  1.0422, -1.9513,  0.4186,  3.3214,  0.8764])\n",
      "example_input_sequence: \n",
      " tensor([[ 0.3446,  0.5199, -2.6133],\n",
      "        [-1.6965, -0.2282,  0.2800],\n",
      "        [ 0.0732,  1.1133,  0.3380],\n",
      "        [ 0.4544,  0.4569, -0.8654],\n",
      "        [ 0.7813, -0.9268,  0.2064],\n",
      "        [-0.3334, -0.0729, -0.0340],\n",
      "        [ 0.9625,  0.3492, -0.9215],\n",
      "        [-0.0562, -0.7015,  1.0367],\n",
      "        [ 1.9218, -0.4025,  0.1239],\n",
      "        [ 1.1648,  0.9234,  1.3873],\n",
      "        [ 1.3750,  0.6596, -0.8048],\n",
      "        [ 0.5656,  0.6104,  0.4669],\n",
      "        [ 1.9507, -1.0631,  1.1404],\n",
      "        [-0.0899, -0.5940, -1.2439],\n",
      "        [-0.1021, -1.0335, -0.1434],\n",
      "        [-0.3173,  0.9671, -0.9911],\n",
      "        [ 0.3016, -0.1073,  0.9985],\n",
      "        [-0.4987,  0.9910, -0.7777],\n",
      "        [ 0.3140,  0.2133, -0.1201],\n",
      "        [ 0.3605, -0.3140, -1.0787]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Don't change this cell\n",
    "'''\n",
    "example_input_size = 3\n",
    "example_hidden_size = 6\n",
    "example_sequence_length = 20\n",
    "\n",
    "torch.manual_seed(0)\n",
    "example_weight_for_hidden_to_hidden = torch.randn([example_hidden_size, example_hidden_size])\n",
    "example_weight_for_input_to_hidden = torch.randn([example_hidden_size, example_input_size])\n",
    "example_bias = torch.randn([example_hidden_size])\n",
    "example_input_sequence = torch.randn([example_sequence_length, example_input_size])\n",
    "\n",
    "print('example_weight_for_hidden_to_hidden: \\n',example_weight_for_hidden_to_hidden)\n",
    "print('example_weight_for_input_to_hidden: \\n',example_weight_for_input_to_hidden)\n",
    "print('example_bias: \\n',example_bias)\n",
    "print('example_input_sequence: \\n',example_input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9752e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_single_step(current_input:torch.Tensor, prev_hidden:torch.Tensor, hh_weight:torch.Tensor, ih_weight:torch.Tensor, bias:torch.Tensor) -> torch.Tensor:\n",
    "  '''\n",
    "  This function \n",
    "  \n",
    "  Arguments:\n",
    "    current_input: Input vector of the current time step. Has a shape of [input_dimension]\n",
    "    prev_hidden: Hidden state from the previous time step. Has a shape of [hidden_dimension]\n",
    "    hh_weight: Weight matrix for from hidden state to hidden state. Has a shape of [hidden_dimension, hidden_dimension]\n",
    "    ih_weight: Weight matrix for from current input to hidden state. Has a shape of [input_dimension, hidden_dimension]\n",
    "    bias: Bias of RNN. Has a shape of [hidden_dimension]\n",
    "  \n",
    "  Outputs:\n",
    "    current hidden: Updated hidden state for the current time step. Has a shape of [hidden_dimension]\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  current_hidden = torch.tanh(torch.matmul(ih_weight, current_input) + torch.matmul(hh_weight, prev_hidden) + bias)\n",
    "  \n",
    "  return current_hidden\n",
    "\n",
    "\n",
    "def initialize_hidden_state_for_single_batch(hidden_dim:int) -> torch.Tensor:\n",
    "  '''\n",
    "  This function returns zero Tensor for a given hidden dimension. This function assumes that the RNN uses single layer and single direction.\n",
    "  \n",
    "  Argument\n",
    "    hidden_dim\n",
    "    \n",
    "  Return\n",
    "    initial_hidden_state: Has a shape of [hidden_dim]\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  initial_hidden_state = torch.zeros(hidden_dim)\n",
    "\n",
    "  return initial_hidden_state\n",
    "\n",
    "\n",
    "initial_hidden = initialize_hidden_state_for_single_batch(example_hidden_size)\n",
    "assert initial_hidden.shape == torch.Size([example_hidden_size])\n",
    "\n",
    "single_output = rnn_single_step(example_input_sequence[0], initial_hidden, example_weight_for_hidden_to_hidden, example_weight_for_input_to_hidden, example_bias)\n",
    "assert torch.allclose(single_output, torch.Tensor([ 0.2690, -0.9982,  0.9929, -0.9535,  1.0000,  0.0081]), atol=1e-4), 'Your output is not correct. Please check your code.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93b86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_for_entire_timestep(input_seq:torch.Tensor, prev_hidden:torch.Tensor, hh_weight:torch.Tensor, ih_weight:torch.Tensor, bias:torch.Tensor) -> tuple:\n",
    "  '''\n",
    "  This function returns the output of RNN for the given 'input_seq', for the given RNN's parameters (hh_weight, ih_weight, and bias)\n",
    "  \n",
    "  Arguments:\n",
    "    input_seq: Sequence of input vector. Has a shape of [number_of_timestep, input_dimension]\n",
    "    prev_hidden: Hidden state from the previous time step. Has a shape of [hidden_dimension]\n",
    "    hh_weight: Weight matrix for from hidden state to hidden state. Has a shape of [hidden_dimension, hidden_dimension]\n",
    "    ih_weight: Weight matrix for from current input to hidden state. Has a shape of [input_dimension, hidden_dimension]\n",
    "    bias: Bias of RNN. Has a shape of [hidden_dimension]\n",
    "\n",
    "  \n",
    "  Return: tuple (output, final_hidden_state)\n",
    "    output (torch.Tensor): Sequence of output hidden state of RNN along input timesteps. Has a a shape of [number_of_timestep, hidden_dimension]\n",
    "    final_hidden_state (torch.Tensor): Hidden state of RNN of the last time step. Has a a shape of [hidden_dimension]\n",
    "    \n",
    "  TODO: Complete this function using your 'rnn_single_step()'\n",
    "  '''\n",
    "  hidden_states = []\n",
    "  number_of_timestep = input_seq.shape[0]\n",
    "  for i in range(number_of_timestep):\n",
    "    prev_hidden = rnn_single_step(input_seq[i], prev_hidden, hh_weight, ih_weight, bias)\n",
    "    hidden_states.append(prev_hidden)\n",
    "  output = torch.stack(hidden_states)\n",
    "  return output, prev_hidden\n",
    "\n",
    "total_output = rnn_for_entire_timestep(example_input_sequence, initial_hidden, example_weight_for_hidden_to_hidden, example_weight_for_input_to_hidden, example_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6557792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed the test cases\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test case\n",
    "'''\n",
    "\n",
    "assert isinstance(total_output, tuple) and len(total_output)==2, \"RNN's output has to be tuple of two tensors\"\n",
    "assert isinstance(total_output[0], torch.Tensor), 'Hidden states has to be a tensor'\n",
    "assert torch.allclose(total_output[0][6], torch.tensor([ 0.8273,  0.5121, -0.5701, -0.9566,  0.9984,  0.5125]), atol= 1e-4), f\"Output value is different: {total_output[0][6]}\"\n",
    "assert torch.allclose(total_output[1], torch.tensor([-0.2121, -0.9892, -0.9953,  0.7993,  1.0000, -0.9995]), atol=1e-4), f\"Output value is different: {total_output[1]}\"\n",
    "\n",
    "print(\"Passed the test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3661e",
   "metadata": {},
   "source": [
    "## Problem 2: Understanding Embedding Layer (10 pts)\n",
    "- Embedding Layer takes categorical indices and return corresponding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc07eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[3, 5],\n",
       "          [2, 1]],\n",
       " \n",
       "         [[0, 9],\n",
       "          [3, 1]],\n",
       " \n",
       "         [[1, 0],\n",
       "          [3, 6]]]),\n",
       " tensor([[[[-1.2633,  0.3500,  0.3081],\n",
       "           [-0.1116, -0.6136,  0.0316]],\n",
       " \n",
       "          [[-0.3160, -2.1152,  0.3223],\n",
       "           [-0.4339,  0.8487,  0.6920]]],\n",
       " \n",
       " \n",
       "         [[[-1.1258, -1.1524, -0.2506],\n",
       "           [ 0.0525,  0.5229,  2.3022]],\n",
       " \n",
       "          [[-1.2633,  0.3500,  0.3081],\n",
       "           [-0.4339,  0.8487,  0.6920]]],\n",
       " \n",
       " \n",
       "         [[[-0.4339,  0.8487,  0.6920],\n",
       "           [-1.1258, -1.1524, -0.2506]],\n",
       " \n",
       "          [[-1.2633,  0.3500,  0.3081],\n",
       "           [-0.4927,  0.2484,  0.4397]]]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomEmbeddingLayer(nn.Module):\n",
    "  def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "    super().__init__()\n",
    "    self.weight = torch.randn(num_embeddings, embedding_dim)\n",
    "    \n",
    "  def forward(self, x:torch.LongTensor):\n",
    "    '''\n",
    "    Argument\n",
    "      x: torch.LongTensor of arbitrary shape, where each element represent categorical index smaller than self.num_embeddings\n",
    "      \n",
    "    Return\n",
    "      out (torch.Tensor): torch.FloatTensor with [shape of x, self.embedding_dim]\n",
    "    \n",
    "    TODO: Complete this function using self.weight\n",
    "    '''\n",
    "    out = self.weight[x]\n",
    "    \n",
    "    return out\n",
    "  \n",
    "torch.manual_seed(0)\n",
    "custom_embedding_layer = CustomEmbeddingLayer(10, example_input_size)\n",
    "random_categorical_input = torch.randint(0,10, [3, 2, 2])\n",
    "out = custom_embedding_layer(random_categorical_input)\n",
    "random_categorical_input, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d1b7d2",
   "metadata": {},
   "source": [
    "## Problem 3: Dataset (20 pts)\n",
    "- You have to declare a path for saving dataset\n",
    "- The dataset has vocabulary information\n",
    "    - For both pitch and duration, we added `'start'` and `'end'` token\n",
    "    - This helps a language model to start the generation or end the generation\n",
    "- You have to implment `__getitem__` of this dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b466ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_path = 'essen_folk/'\n",
    "# '''\n",
    "# You can download the dataset like this, but it will take too much time in Colab\n",
    "\n",
    "# essen = muspy.EssenFolkSongDatabase(your_path, download_and_extract=True)\n",
    "# essen.convert()\n",
    "# '''\n",
    "# !pip install --upgrade gdown\n",
    "# !gdown 1HMHgPifMFgRtIiLJsTb3ULqbxJx4xpQY # If it doesn't work, you have to upgrade gdown by !pip install --upgrade gdown\n",
    "\n",
    "# # Following code will automatically unzip te dataset to essen_folk/\n",
    "# !unzip -oq essen_converted.zip  # option: overwrite, quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba4a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip downloading as the `.muspy.success` file is found.\n",
      "Skip extracting as the `.muspy.success` file is found.\n",
      "Skip conversion as the `.muspy.success` file is found.\n"
     ]
    }
   ],
   "source": [
    "class MelodyDataset:\n",
    "  def __init__(self, muspy_dataset, vocabs=None):\n",
    "    '''\n",
    "    The dataset takes vocabs as an argument. If vocabs is None, the dataset will automatically generate vocabs from the given dataset.\n",
    "    This is useful when you want to use the same vocabs for training and test dataset.\n",
    "    \n",
    "    '''\n",
    "    self.dataset = muspy_dataset\n",
    "    \n",
    "    if vocabs is None:\n",
    "      '''\n",
    "      Even though you don't have to add 'pad' when if you only use PackedSequence, we will add 'pad' to the vocabulary just in case using paddings.\n",
    "      '''\n",
    "      self.idx2pitch, self.idx2dur = self._get_vocab_info()\n",
    "      self.idx2pitch = ['pad', 'start', 'end'] + self.idx2pitch \n",
    "      self.idx2dur = ['pad', 'start', 'end'] + self.idx2dur\n",
    "      self.pitch2idx = {x:i for i, x in enumerate(self.idx2pitch)}\n",
    "      self.dur2idx = {x:i for i, x in enumerate(self.idx2dur)}\n",
    "      \n",
    "    else:\n",
    "      self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx = vocabs\n",
    "    \n",
    "  def _get_vocab_info(self):\n",
    "    entire_pitch = []\n",
    "    entire_dur = []\n",
    "    for note_rep in self.dataset:\n",
    "      pitch_in_piece = note_rep[:, 1]\n",
    "      dur_in_piece = note_rep[:, 2]\n",
    "      entire_pitch += pitch_in_piece.tolist()\n",
    "      entire_dur += dur_in_piece.tolist()\n",
    "    return list(set(entire_pitch)), list(set(entire_dur))\n",
    "  \n",
    "  def get_vocabs(self):\n",
    "    return self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "  \n",
    "  def __getitem__(self, idx:int):\n",
    "    '''\n",
    "    This dataset class returns melody information as a tensor with shape of [num_notes, 2 (pitch, duration)].\n",
    "    \n",
    "    To train a melody language model, you have to provide a sequence of original note, and a sequence of next note for given original note.\n",
    "    In other word, melody[i+1] has to be the shifted_melody[i], so that melody[i]'s next note can be retrieved by shifted_melody[i]\n",
    "    (Remember, language model is trained to predict the next upcoming word)\n",
    "    \n",
    "    Also, to make genration easier, we usually add 'start' token at the beginning of sequence, and 'end' token at the end of the sequence.\n",
    "    With these tokens, we can make the model recognize where is the start and end of the sequence explicitly.\n",
    "    \n",
    "    You have to add these tokens to the note sequence at this step.\n",
    "    \n",
    "    Argument:\n",
    "      idx (int): Index of data sample in the dataset\n",
    "    \n",
    "    Returns:\n",
    "      melody (torch.LongTensor): Sequence of [categorical_index_of_pitch, categorical_index_of_duration]\n",
    "                                 Has a shape of [1 (start_token) + num_notes, 2 (pitch, dur)]. \n",
    "                                 The first element of the sequence has to be the index for 'start' token for both pitch and duration.\n",
    "                                 The melody should not include 'end' token \n",
    "                                 (Because we don't have to predict next note if we know that current note is 'end' token)\n",
    "      shifted_melody (torch.LongTensor): Sequence of [categorical_index_of_pitch, categorical_index_of_duration]\n",
    "                                         Has a shape of [num_notes + 1 (end_token), 2 (pitch, dur)]\n",
    "                                         The i'th note of shifted melody has to be the same with (i+1)'th note of melody\n",
    "                                         The shifted melody should not include 'start' token \n",
    "                                         (Because we never get a 'start' token after a note)\n",
    "\n",
    "    TODO: Complete this function\n",
    "    '''\n",
    "    note_rep = self.dataset[idx]\n",
    "    pitch_in_piece = note_rep[:, 1]\n",
    "    dur_in_piece = note_rep[:, 2]\n",
    "\n",
    "    start_idx_tensor = torch.LongTensor([self.pitch2idx['start'], self.dur2idx['start']])\n",
    "    end_idx_tensor = torch.LongTensor([self.pitch2idx['end'], self.dur2idx['end']])\n",
    "    pitch_and_duration_idx_tensor_list = [torch.LongTensor([self.pitch2idx[p], self.dur2idx[d]]) for p, d in zip(pitch_in_piece, dur_in_piece)]\n",
    "    \n",
    "    melody = torch.stack([start_idx_tensor] + pitch_and_duration_idx_tensor_list)\n",
    "    shifted_melody = torch.stack(pitch_and_duration_idx_tensor_list + [end_idx_tensor])\n",
    "    \n",
    "    return melody, shifted_melody\n",
    "\n",
    "your_path = 'essen_folk/'\n",
    "essen = muspy.EssenFolkSongDatabase(your_path, download_and_extract=True)\n",
    "essen.convert()\n",
    "\n",
    "essen_entire = essen.to_pytorch_dataset(representation='note')\n",
    "essen_split = essen.to_pytorch_dataset(representation='note', splits=(0.8, 0.1, 0.1), random_state=0)\n",
    "entire_set = MelodyDataset(essen_entire)\n",
    "\n",
    "vocabs = entire_set.get_vocabs()\n",
    "train_set = MelodyDataset(essen_split['train'], vocabs=vocabs)\n",
    "valid_set = MelodyDataset(essen_split['validation'], vocabs=vocabs)\n",
    "test_set = MelodyDataset(essen_split['test'], vocabs=vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0372bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test cases\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "To check the MelodyDataset implementation\n",
    "'''\n",
    "\n",
    "assert len(train_set[0]) == 2, \"You have to return two variables at __getitem__\"\n",
    "assert train_set[0][0].shape == train_set[0][1].shape, \"Shape of Melody and Shifted melody has to be the same\"\n",
    "\n",
    "assert (train_set[0][0][0] == torch.LongTensor([1, 1])).all(), \"You have to add start token at the beginning of melody\"\n",
    "assert (train_set[0][1][-1] == torch.LongTensor([2, 2])).all(), \"You have to add end token at the end of melody\"\n",
    "\n",
    "assert (train_set[0][0][-1] == torch.LongTensor([15, 29])).all(), \"Last part of melody must not include the end token\"\n",
    "assert (train_set[0][1][0] == torch.LongTensor([27, 19])).all(),  \"First part of shifted melody must not include the start token\"\n",
    "\n",
    "assert (train_set[20][0][1:] == train_set[20][1][:-1]).all(), \"Check the melody shift\"\n",
    "\n",
    "print(\"Passed test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673c342",
   "metadata": {},
   "source": [
    "## PackedSequence\n",
    "- After implementing Dataset, we have to declare DataLoader that groups several training samples as a single batch\n",
    "- However, we cannot batchify the melodies in straightforward way, because the length of each melody is different\n",
    "- In this problem, you will learn about how to handle sequences of different length as a batch\n",
    "\n",
    "- You can also refer [a video lecture](https://youtu.be/IQf1zu6jdCU) in Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83754613",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# '''\n",
    "# This cell will make error, because the length of each sample is different to each other\n",
    "# '''\n",
    "\n",
    "# train_loader = DataLoader(train_set, batch_size=8)\n",
    "# batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58708cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To handle that problem, you have to make your collate function \n",
    "'''\n",
    "def your_collate_function(raw_batch):\n",
    "  '''\n",
    "  You can make your own function to handle the batch\n",
    "  '''\n",
    "  max_len = max([len(melody) for melody, _ in raw_batch])\n",
    "  batch_size = len(raw_batch)\n",
    "  batched_melody = torch.zeros([batch_size, max_len, 2], dtype=torch.long)\n",
    "  batched_shifted_melody = torch.zeros([batch_size, max_len, 2], dtype=torch.long)\n",
    "\n",
    "  for i, (melody, shifted_melody) in enumerate(raw_batch):\n",
    "    batched_melody[i, :len(melody)] = melody\n",
    "    batched_shifted_melody[i, :len(shifted_melody)] = shifted_melody\n",
    "\n",
    "  return batched_melody, batched_shifted_melody\n",
    "  \n",
    "batch_size = 8\n",
    "raw_batch = [train_set[i] for i in range(batch_size)] # This is the input for the collate function\n",
    "batch = your_collate_function(raw_batch)\n",
    "\n",
    "'''\n",
    "This is what the 'collate_fn' does in DataLoader\n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=your_collate_function)\n",
    "batch_by_loader = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0a209",
   "metadata": {},
   "source": [
    "#### Pad Sequence and Pack Sequence\n",
    "In PyTorch, there are two ways to batchify a group of sequence with different length.\n",
    "- `torch.nn.utils.rnn.pad_sequence`\n",
    "    - This function takes list of tensors with different length and return padded sequence\n",
    "    - Padding is adding some constant number as a PAD token to match the length of short sequence to the maximum length\n",
    "        - e.g. If there are sequence of length (3,7,4), we can add 4 zeros to sequence with length 3, 3 zeros to sequence with length 4 to make them length 7\n",
    "    - In default, we use 0 for padding (zero padding)\n",
    "    - The result \n",
    "- `torch.nn.utils.rnn.pack_sequence`\n",
    "    - pad_sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2a21e",
   "metadata": {},
   "source": [
    "Below cells show the example of pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24424475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  2.],\n",
       "        [ 1.,  6.,  3.],\n",
       "        [ 2.,  8.,  4.],\n",
       "        [ 0., 12.,  3.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  2.,  0.],\n",
       "        [ 0.,  3.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence, PackedSequence\n",
    "short = torch.Tensor([0, 1, 2])\n",
    "long = torch.Tensor([3, 6, 8, 12, 1, 2, 3])\n",
    "middle = torch.Tensor([2, 3, 4, 3, 0])\n",
    "\n",
    "pad_sequence([short, long, middle], batch_first=False)  # T x N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99aec137",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  0.,  0.,  0.,  0.],\n",
       "        [ 3.,  6.,  8., 12.,  1.,  2.,  3.],\n",
       "        [ 2.,  3.,  4.,  3.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default value of batch_first in pad_sequence is False.\n",
    "# So you have to always be careful not to miss batch_first=True in pad_sequence, if you use batch_first=True for your RNN layer.\n",
    "pad_sequence([short, long, middle], batch_first=True)  # N x T "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b420b0",
   "metadata": {},
   "source": [
    "1) However, the problem is that you can't figure out whether the 0 at the end of each sequence is a padded one, or was included in the original sequence\n",
    "- e.g. `[2, 3, 4, 3, 0]` becomes `[ 2,  3,  4,  5,  0,  0,  0]`. Now we don't know how many zeros were added for padding\n",
    "\n",
    "2) Also, if you run RNN for this padded sequence, RNN will calculate for the padded part also.\n",
    "- RNN doesn't know whether it is padded data, or existing data\n",
    "- This makes computation slower\n",
    "\n",
    "3) If you want to use bi-directional, which also reads the sequence from backward, paddings can make the result different.\n",
    "\n",
    "To solve this issue, we use PackedSequence, by using `pack_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f553adab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([ 3.,  2.,  0.,  6.,  3.,  1.,  8.,  4.,  2., 12.,  3.,  1.,  0.,  2.,\n",
       "         3.]), batch_sizes=tensor([3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_sequence = pack_sequence([short, long, middle], enforce_sorted=False)\n",
    "packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c04cfe",
   "metadata": {},
   "source": [
    "`PackedSequence` has `data` and `batch_sizes`\n",
    "- `data` contains the flattened value of given batch\n",
    "    - To optimize the computation, the sequences have to be sorted by descending of length\n",
    "- `batch_sizes` represents how many valid batch sample exists for each time step\n",
    "    - `[3, 3, 3, 2, 2, 1, 1]` means that there are 3 sequences for first three time steps, and then 2 sequences for next two steps, and then only 1 sequence for next two steps.\n",
    "- `sorted_indices` shows how the sorted sequences can be converted to original order.\n",
    "    - `[1,2,0]` means that \n",
    "        - the 0th sequence in the sorted sequences (the longest one) was indexed as 1 in the original input batch\n",
    "        - the 1st sequence in the sorted sequences (`middle`) was indexed as 2 in the original input batch\n",
    "        - the 2nd sequence in the sorted sequences (`short`) was index as 0 in the original input batch\n",
    "- `unsorted_indices` shows how the original sequences are sorted.\n",
    "    - `[2,0,1]` means that\n",
    "        - the 0th sequence in the original input was sorted as 2nd in the sorted sequences\n",
    "        \n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc405b5",
   "metadata": {},
   "source": [
    "If you feed PackedSequence to RNN (or LSTM, GRU), it will return PackedSequence with same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a65589f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of output of RNN for PackedSequence: <class 'torch.nn.utils.rnn.PackedSequence'>\n",
      "Type of last_hidden of RNN for PackedSequence: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "rnn_layer = nn.GRU(1, 1)\n",
    "packed_sequence = pack_sequence([short.unsqueeze(1), long.unsqueeze(1), middle.unsqueeze(1)], enforce_sorted=False)\n",
    "out, last_hidden = rnn_layer(packed_sequence)\n",
    "\n",
    "print(f\"Type of output of RNN for PackedSequence: {type(out)}\")\n",
    "print(f\"Type of last_hidden of RNN for PackedSequence: {type(last_hidden)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df707e",
   "metadata": {},
   "source": [
    "- RNN or its family of PyTorch can automatically handle `PackedSequence`\n",
    "- However, other layers like `nn.Embedding` or `nn.Linear` cannot take `PackedSequence` as its input\n",
    "- There are two ways to feed `PackedSequence` to these layers\n",
    "    - First, convert PackedSequence to ordinary torch.Tensor by `torch.nn.utils.rnn.pad_packed_sequence`\n",
    "        - This will convert PackedSequence to a tensor of sequneces with same length but different padding\n",
    "    - The other way is to feed only PackedSequence.data, and then declaring new PackedSequence with the output as `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e983f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# This will make error, because other layers cannot handle PackedSequence\n",
    "# '''\n",
    "test_linear_layer = nn.Linear(in_features=1, out_features=2)\n",
    "# test_linear_layer(packed_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1754e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padded sequence generated from packed sequence (squeezed for printing): \n",
      " tensor([[ 0.,  3.,  2.],\n",
      "        [ 1.,  6.,  3.],\n",
      "        [ 2.,  8.,  4.],\n",
      "        [ 0., 12.,  3.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  2.,  0.],\n",
      "        [ 0.,  3.,  0.]])\n",
      "\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \n",
      " tensor([3, 7, 5])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "One way to to this is using torch.nn.utils.rnn.pad_packed_sequence to convert PackedSequence to ordinary tensor\n",
    "'''\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "padded_sequence, batch_lengths = pad_packed_sequence(packed_sequence)\n",
    "print(f'The padded sequence generated from packed sequence (squeezed for printing): \\n {padded_sequence.squeeze()}')\n",
    "print(f'\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \\n {batch_lengths}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7871b5da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of feeding padded_sequence to a linear layer: tensor([[[ 0.8702,  0.8824],\n",
      "         [ 2.0492,  3.3680],\n",
      "         [ 1.6562,  2.5395]],\n",
      "\n",
      "        [[ 1.2632,  1.7109],\n",
      "         [ 3.2283,  5.8537],\n",
      "         [ 2.0492,  3.3680]],\n",
      "\n",
      "        [[ 1.6562,  2.5395],\n",
      "         [ 4.0143,  7.5108],\n",
      "         [ 2.4422,  4.1966]],\n",
      "\n",
      "        [[ 0.8702,  0.8824],\n",
      "         [ 5.5863, 10.8249],\n",
      "         [ 2.0492,  3.3680]],\n",
      "\n",
      "        [[ 0.8702,  0.8824],\n",
      "         [ 1.2632,  1.7109],\n",
      "         [ 0.8702,  0.8824]],\n",
      "\n",
      "        [[ 0.8702,  0.8824],\n",
      "         [ 1.6562,  2.5395],\n",
      "         [ 0.8702,  0.8824]],\n",
      "\n",
      "        [[ 0.8702,  0.8824],\n",
      "         [ 2.0492,  3.3680],\n",
      "         [ 0.8702,  0.8824]]], grad_fn=<ViewBackward0>)\n",
      "Shape: torch.Size([7, 3, 2])\n",
      "Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now you can feed padded sequence to linear layer.\n",
    "'''\n",
    "\n",
    "linear_output = test_linear_layer(padded_sequence)\n",
    "print(f\"Output of feeding padded_sequence to a linear layer: {linear_output}\")\n",
    "print(f\"Shape: {linear_output.shape}\")\n",
    "print(\"Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71d104d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 2.0492,  3.3680],\n",
       "        [ 1.6562,  2.5395],\n",
       "        [ 0.8702,  0.8824],\n",
       "        [ 3.2283,  5.8537],\n",
       "        [ 2.0492,  3.3680],\n",
       "        [ 1.2632,  1.7109],\n",
       "        [ 4.0143,  7.5108],\n",
       "        [ 2.4422,  4.1966],\n",
       "        [ 1.6562,  2.5395],\n",
       "        [ 5.5863, 10.8249],\n",
       "        [ 2.0492,  3.3680],\n",
       "        [ 1.2632,  1.7109],\n",
       "        [ 0.8702,  0.8824],\n",
       "        [ 1.6562,  2.5395],\n",
       "        [ 2.0492,  3.3680]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "You can make the output as a PackedSequence, by using torch.nn.utils.rnn.pack_padded_sequence\n",
    "'''\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "re_packed_sequence = pack_padded_sequence(linear_output, batch_lengths, enforce_sorted=False)\n",
    "re_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a869f708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 2.0492,  3.3680],\n",
       "        [ 1.6562,  2.5395],\n",
       "        [ 0.8702,  0.8824],\n",
       "        [ 3.2283,  5.8537],\n",
       "        [ 2.0492,  3.3680],\n",
       "        [ 1.2632,  1.7109],\n",
       "        [ 4.0143,  7.5108],\n",
       "        [ 2.4422,  4.1966],\n",
       "        [ 1.6562,  2.5395],\n",
       "        [ 5.5863, 10.8249],\n",
       "        [ 2.0492,  3.3680],\n",
       "        [ 1.2632,  1.7109],\n",
       "        [ 0.8702,  0.8824],\n",
       "        [ 1.6562,  2.5395],\n",
       "        [ 2.0492,  3.3680]], grad_fn=<AddmmBackward0>), batch_sizes=tensor([3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Another way to do it is using PackedSequence.data\n",
    "'''\n",
    "\n",
    "linear_out_pack = test_linear_layer(packed_sequence.data)\n",
    "packed_sequence_after_linear = PackedSequence(linear_out_pack, packed_sequence.batch_sizes, packed_sequence.sorted_indices, packed_sequence.unsorted_indices)\n",
    "packed_sequence_after_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef97f8e",
   "metadata": {},
   "source": [
    "## Problem 4: Implement pack_collate(), (20 pts)\n",
    "- Implement a collate function that returns PackedSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "518ed3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
    "\n",
    "def pack_collate(raw_batch:list):\n",
    "  '''\n",
    "  This function takes a list of data, and returns two PackedSequences\n",
    "  \n",
    "  Argument\n",
    "    raw_batch: A list of MelodyDataset[idx]. Each item in the list is a tuple of (melody, shifted_melody)\n",
    "               melody and shifted_melody has a shape of [num_notes (+1 if you don't consider \"start\" and \"end\" token as note), 2]\n",
    "  Returns\n",
    "    packed_melody (torch.nn.utils.rnn.PackedSequence)\n",
    "    packed_shifted_melody (torch.nn.utils.rnn.PackedSequence)\n",
    "\n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  melody_list = [melody for melody, _ in raw_batch]\n",
    "  shifted_melody_list = [shifted_melody for _, shifted_melody in raw_batch]\n",
    "\n",
    "  packed_melody = pack_sequence(melody_list, enforce_sorted=False)\n",
    "  packed_shifted_melody = pack_sequence(shifted_melody_list, enforce_sorted=False)\n",
    "  \n",
    "  return packed_melody, packed_shifted_melody\n",
    "\n",
    "raw_batch = [train_set[i] for i in range(batch_size)]\n",
    "packed_melody, packed_shifted_melody = pack_collate(raw_batch)\n",
    "# packed_melody, packed_shifted_melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff1e46bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all the test cases\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test whether you have implemented pack_collate correctly\n",
    "'''\n",
    "\n",
    "assert isinstance(packed_melody, PackedSequence)\n",
    "assert isinstance(packed_shifted_melody, PackedSequence)\n",
    "\n",
    "assert packed_melody.data.shape==packed_shifted_melody.data.shape\n",
    "\n",
    "print(\"Passed all the test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d182eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[ 1,  1],\n",
       "         [ 1,  1],\n",
       "         [ 1,  1],\n",
       "         ...,\n",
       "         [24, 29],\n",
       "         [15, 29],\n",
       "         [22, 15]]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63,\n",
       "         63, 63, 62, 62, 61, 61, 60, 58, 57, 57, 56, 55, 55, 53, 52, 51, 49, 46,\n",
       "         45, 43, 42, 40, 38, 36, 35, 34, 33, 31, 25, 24, 24, 24, 24, 24, 23, 22,\n",
       "         22, 22, 22, 20, 18, 18, 18, 16, 16, 16, 16, 14, 14, 14, 14, 14, 14, 13,\n",
       "         12, 12, 11, 11, 10,  9,  9,  8,  8,  7,  5,  5,  5,  4,  4,  3,  2,  2,\n",
       "          2,  2,  2,  1]), sorted_indices=tensor([ 6, 30, 33, 57,  7, 56, 54, 46, 52, 15, 63, 24, 61, 14, 60, 16, 20, 49,\n",
       "         34, 32, 42, 47, 59,  2, 31, 11, 39, 48,  4, 35, 36, 26, 51, 10, 50, 27,\n",
       "          8, 18, 53, 43, 37, 38,  1,  0, 55, 17, 45, 21, 58, 44,  5, 62, 12, 22,\n",
       "         23, 40, 41, 25, 28, 29,  3,  9, 13, 19]), unsorted_indices=tensor([43, 42, 23, 60, 28, 50,  0,  4, 36, 61, 33, 25, 52, 62, 13,  9, 15, 45,\n",
       "         37, 63, 16, 47, 53, 54, 11, 57, 31, 35, 58, 59,  1, 24, 19,  2, 18, 29,\n",
       "         30, 40, 41, 26, 55, 56, 20, 39, 49, 46,  7, 21, 27, 17, 34, 32,  8, 38,\n",
       "          6, 44,  5,  3, 48, 22, 14, 12, 51, 10])),\n",
       " PackedSequence(data=tensor([[22, 29],\n",
       "         [29,  8],\n",
       "         [17, 19],\n",
       "         ...,\n",
       "         [22, 15],\n",
       "         [ 2,  2],\n",
       "         [ 2,  2]]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63,\n",
       "         63, 63, 62, 62, 61, 61, 60, 58, 57, 57, 56, 55, 55, 53, 52, 51, 49, 46,\n",
       "         45, 43, 42, 40, 38, 36, 35, 34, 33, 31, 25, 24, 24, 24, 24, 24, 23, 22,\n",
       "         22, 22, 22, 20, 18, 18, 18, 16, 16, 16, 16, 14, 14, 14, 14, 14, 14, 13,\n",
       "         12, 12, 11, 11, 10,  9,  9,  8,  8,  7,  5,  5,  5,  4,  4,  3,  2,  2,\n",
       "          2,  2,  2,  1]), sorted_indices=tensor([ 6, 30, 33, 57,  7, 56, 54, 46, 52, 15, 63, 24, 61, 14, 60, 16, 20, 49,\n",
       "         34, 32, 42, 47, 59,  2, 31, 11, 39, 48,  4, 35, 36, 26, 51, 10, 50, 27,\n",
       "          8, 18, 53, 43, 37, 38,  1,  0, 55, 17, 45, 21, 58, 44,  5, 62, 12, 22,\n",
       "         23, 40, 41, 25, 28, 29,  3,  9, 13, 19]), unsorted_indices=tensor([43, 42, 23, 60, 28, 50,  0,  4, 36, 61, 33, 25, 52, 62, 13,  9, 15, 45,\n",
       "         37, 63, 16, 47, 53, 54, 11, 57, 31, 35, 58, 59,  1, 24, 19,  2, 18, 29,\n",
       "         30, 40, 41, 26, 55, 56, 20, 39, 49, 46,  7, 21, 27, 17, 34, 32,  8, 38,\n",
       "          6, 44,  5,  3, 48, 22, 14, 12, 51, 10])))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, collate_fn=pack_collate, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=128, collate_fn=pack_collate, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=128, collate_fn=pack_collate, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73263352",
   "metadata": {},
   "source": [
    "## Problem 5: Define Melody Language Model (25 pts)\n",
    "- In this problem, you have to define a Language Model for model\n",
    "    - It is almost same as an ordinary language model for natural language processing\n",
    "    - The key difference is that the melody language model has to predict pitch **and** duration\n",
    "- Complete the model step-by-step\n",
    "    - Complete each function and test the function with the cells below\n",
    "    - `get_concat_embedding()` makes concatenated embedding for each note given pitch and duration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "893efba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[[0.0278, 0.0249, 0.0251,  ..., 0.0262, 0.0204, 0.0252],\n",
       "          [0.0281, 0.0258, 0.0240,  ..., 0.0270, 0.0198, 0.0254],\n",
       "          [0.0284, 0.0264, 0.0230,  ..., 0.0278, 0.0193, 0.0250],\n",
       "          ...,\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244],\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244],\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244]],\n",
       " \n",
       "         [[0.0278, 0.0249, 0.0251,  ..., 0.0262, 0.0204, 0.0252],\n",
       "          [0.0276, 0.0257, 0.0245,  ..., 0.0265, 0.0198, 0.0252],\n",
       "          [0.0276, 0.0261, 0.0236,  ..., 0.0267, 0.0194, 0.0246],\n",
       "          ...,\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244],\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244],\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244]],\n",
       " \n",
       "         [[0.0278, 0.0249, 0.0251,  ..., 0.0262, 0.0204, 0.0252],\n",
       "          [0.0277, 0.0256, 0.0246,  ..., 0.0265, 0.0197, 0.0256],\n",
       "          [0.0274, 0.0260, 0.0237,  ..., 0.0270, 0.0193, 0.0253],\n",
       "          ...,\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244],\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244],\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0278, 0.0249, 0.0251,  ..., 0.0262, 0.0204, 0.0252],\n",
       "          [0.0277, 0.0255, 0.0248,  ..., 0.0263, 0.0196, 0.0253],\n",
       "          [0.0275, 0.0257, 0.0244,  ..., 0.0263, 0.0191, 0.0249],\n",
       "          ...,\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244],\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244],\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244]],\n",
       " \n",
       "         [[0.0278, 0.0249, 0.0251,  ..., 0.0262, 0.0204, 0.0252],\n",
       "          [0.0278, 0.0254, 0.0250,  ..., 0.0267, 0.0198, 0.0258],\n",
       "          [0.0276, 0.0255, 0.0250,  ..., 0.0272, 0.0195, 0.0259],\n",
       "          ...,\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244],\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244],\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244]],\n",
       " \n",
       "         [[0.0278, 0.0249, 0.0251,  ..., 0.0262, 0.0204, 0.0252],\n",
       "          [0.0279, 0.0252, 0.0248,  ..., 0.0267, 0.0196, 0.0255],\n",
       "          [0.0280, 0.0257, 0.0238,  ..., 0.0275, 0.0192, 0.0254],\n",
       "          ...,\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244],\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244],\n",
       "          [0.0272, 0.0242, 0.0253,  ..., 0.0260, 0.0214, 0.0244]]],\n",
       "        grad_fn=<SoftmaxBackward0>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63,\n",
       "         63, 63, 62, 62, 61, 61, 60, 58, 57, 57, 56, 55, 55, 53, 52, 51, 49, 46,\n",
       "         45, 43, 42, 40, 38, 36, 35, 34, 33, 31, 25, 24, 24, 24, 24, 24, 23, 22,\n",
       "         22, 22, 22, 20, 18, 18, 18, 16, 16, 16, 16, 14, 14, 14, 14, 14, 14, 13,\n",
       "         12, 12, 11, 11, 10,  9,  9,  8,  8,  7,  5,  5,  5,  4,  4,  3,  2,  2,\n",
       "          2,  2,  2,  1]), sorted_indices=tensor([ 6, 30, 33, 57,  7, 56, 54, 46, 52, 15, 63, 24, 61, 14, 60, 16, 20, 49,\n",
       "         34, 32, 42, 47, 59,  2, 31, 11, 39, 48,  4, 35, 36, 26, 51, 10, 50, 27,\n",
       "          8, 18, 53, 43, 37, 38,  1,  0, 55, 17, 45, 21, 58, 44,  5, 62, 12, 22,\n",
       "         23, 40, 41, 25, 28, 29,  3,  9, 13, 19]), unsorted_indices=tensor([43, 42, 23, 60, 28, 50,  0,  4, 36, 61, 33, 25, 52, 62, 13,  9, 15, 45,\n",
       "         37, 63, 16, 47, 53, 54, 11, 57, 31, 35, 58, 59,  1, 24, 19,  2, 18, 29,\n",
       "         30, 40, 41, 26, 55, 56, 20, 39, 49, 46,  7, 21, 27, 17, 34, 32,  8, 38,\n",
       "          6, 44,  5,  3, 48, 22, 14, 12, 51, 10])),\n",
       " PackedSequence(data=tensor([[[0.0231, 0.0236, 0.0203,  ..., 0.0186, 0.0229, 0.0216],\n",
       "          [0.0241, 0.0239, 0.0195,  ..., 0.0187, 0.0228, 0.0212],\n",
       "          [0.0246, 0.0244, 0.0187,  ..., 0.0191, 0.0231, 0.0208],\n",
       "          ...,\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213],\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213],\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213]],\n",
       " \n",
       "         [[0.0231, 0.0236, 0.0203,  ..., 0.0186, 0.0229, 0.0216],\n",
       "          [0.0238, 0.0235, 0.0198,  ..., 0.0188, 0.0226, 0.0213],\n",
       "          [0.0239, 0.0237, 0.0195,  ..., 0.0188, 0.0225, 0.0206],\n",
       "          ...,\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213],\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213],\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213]],\n",
       " \n",
       "         [[0.0231, 0.0236, 0.0203,  ..., 0.0186, 0.0229, 0.0216],\n",
       "          [0.0233, 0.0239, 0.0199,  ..., 0.0189, 0.0227, 0.0216],\n",
       "          [0.0238, 0.0244, 0.0193,  ..., 0.0192, 0.0226, 0.0211],\n",
       "          ...,\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213],\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213],\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0231, 0.0236, 0.0203,  ..., 0.0186, 0.0229, 0.0216],\n",
       "          [0.0229, 0.0237, 0.0204,  ..., 0.0186, 0.0225, 0.0217],\n",
       "          [0.0225, 0.0240, 0.0205,  ..., 0.0185, 0.0221, 0.0219],\n",
       "          ...,\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213],\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213],\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213]],\n",
       " \n",
       "         [[0.0231, 0.0236, 0.0203,  ..., 0.0186, 0.0229, 0.0216],\n",
       "          [0.0235, 0.0235, 0.0197,  ..., 0.0188, 0.0231, 0.0214],\n",
       "          [0.0238, 0.0234, 0.0190,  ..., 0.0191, 0.0236, 0.0212],\n",
       "          ...,\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213],\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213],\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213]],\n",
       " \n",
       "         [[0.0231, 0.0236, 0.0203,  ..., 0.0186, 0.0229, 0.0216],\n",
       "          [0.0236, 0.0239, 0.0198,  ..., 0.0190, 0.0225, 0.0217],\n",
       "          [0.0246, 0.0241, 0.0189,  ..., 0.0193, 0.0224, 0.0211],\n",
       "          ...,\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213],\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213],\n",
       "          [0.0229, 0.0237, 0.0205,  ..., 0.0188, 0.0231, 0.0213]]],\n",
       "        grad_fn=<SoftmaxBackward0>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63,\n",
       "         63, 63, 62, 62, 61, 61, 60, 58, 57, 57, 56, 55, 55, 53, 52, 51, 49, 46,\n",
       "         45, 43, 42, 40, 38, 36, 35, 34, 33, 31, 25, 24, 24, 24, 24, 24, 23, 22,\n",
       "         22, 22, 22, 20, 18, 18, 18, 16, 16, 16, 16, 14, 14, 14, 14, 14, 14, 13,\n",
       "         12, 12, 11, 11, 10,  9,  9,  8,  8,  7,  5,  5,  5,  4,  4,  3,  2,  2,\n",
       "          2,  2,  2,  1]), sorted_indices=tensor([ 6, 30, 33, 57,  7, 56, 54, 46, 52, 15, 63, 24, 61, 14, 60, 16, 20, 49,\n",
       "         34, 32, 42, 47, 59,  2, 31, 11, 39, 48,  4, 35, 36, 26, 51, 10, 50, 27,\n",
       "          8, 18, 53, 43, 37, 38,  1,  0, 55, 17, 45, 21, 58, 44,  5, 62, 12, 22,\n",
       "         23, 40, 41, 25, 28, 29,  3,  9, 13, 19]), unsorted_indices=tensor([43, 42, 23, 60, 28, 50,  0,  4, 36, 61, 33, 25, 52, 62, 13,  9, 15, 45,\n",
       "         37, 63, 16, 47, 53, 54, 11, 57, 31, 35, 58, 59,  1, 24, 19,  2, 18, 29,\n",
       "         30, 40, 41, 26, 55, 56, 20, 39, 49, 46,  7, 21, 27, 17, 34, 32,  8, 38,\n",
       "          6, 44,  5,  3, 48, 22, 14, 12, 51, 10])))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import PackedSequence\n",
    "\n",
    "class MelodyLanguageModel(nn.Module):\n",
    "  def __init__(self, hidden_size, embed_size, vocabs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx = vocabs\n",
    "    self.hidden_size = hidden_size\n",
    "    self.embed_size = embed_size\n",
    "    self.num_pitch = len(self.idx2pitch)\n",
    "    self.num_dur = len(self.idx2dur)\n",
    "    self.num_layers = 3\n",
    "    \n",
    "    '''\n",
    "    TODO: Declare four modules. Please follow the name strictly.\n",
    "      1) self.pitch_embedder: nn.Embedding layer that embed pitch category index to a vector with size of 'embed_size'\n",
    "      2) self.dur_embedder = nn.Embedding layer that embed duration category index to a vector with size of 'embed_size'\n",
    "      3) self.rnn = nn.GRU layer that takes concatenated_embedding and has a hidden size of 'hidden_size', num_layers of self.num_layers, and batch_first=True\n",
    "      4) self.final_layer = nn.Linear layer that takes self.rnn's output and convert it to logits (that can be used as input of softmax) of pitch + duration\n",
    "    '''\n",
    "    self.pitch_embedder = nn.Embedding(self.num_pitch, self.embed_size)\n",
    "    self.dur_embedder = nn.Embedding(self.num_dur, self.embed_size)\n",
    "    self.rnn = nn.GRU(input_size=self.embed_size*2, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
    "    self.final_layer = nn.Linear(self.hidden_size, self.num_pitch + self.num_dur)\n",
    "    \n",
    "  def get_concat_embedding(self, input_seq):\n",
    "    '''\n",
    "    This function returns concatenated pitch embedding and duration embedding for a given input seq\n",
    "    \n",
    "    Arguments:\n",
    "      input_seq: A batch of melodies represented as a sequence of vector (pitch_idx, dur_idx). \n",
    "                 Has a shape of [num_batch, num_timesteps (num_notes), 2(pitch, dur)], or [num_timesteps (num_notes), 2]\n",
    "                 벡터 (pitch_idx, dur_idx)의 시퀀스로 표현된 멜로디들의 집합으로 이루어진 배치. \n",
    "                 Shape은 [배치 샘플 수, 타임스텝의 수 (==음표의 수), 2 (음고, 길이)] 혹은 [타임스텝의 수 (num_notes), 2]\n",
    "    Return:\n",
    "      concat_embedding: A batch of sequence of concatenated embedding of pitch embedding and duration embedding.\n",
    "                        Has a shape of [num_batch, num_timesteps (num_notes), embedding_size * 2]\n",
    "                        Each vector of time t is [pitch_embedding ; duration_embedding] (concatenation)\n",
    "                        \n",
    "                        pitch embedding is the output of an nn.Embedding layer of given note pitch index\n",
    "                        duration embedding is the output of an nn.Embedding layer of given note duration index\n",
    "    \n",
    "    TODO: Complete this function using self.pitch_embedder and self.dur_embedder\n",
    "    You can use torch.cat to concatenate two tensors or vectors\n",
    "    '''\n",
    "    if len(input_seq.shape) == 2:\n",
    "      input_seq = torch.unsqueeze(input_seq, 0)\n",
    "\n",
    "    pitch_embedding = self.pitch_embedder(input_seq[:,:,0])\n",
    "    dur_embedding = self.dur_embedder(input_seq[:,:,1])\n",
    "\n",
    "    concat_embedding = torch.cat([pitch_embedding, dur_embedding], dim=-1)\n",
    "    assert concat_embedding.shape == torch.Size([input_seq.shape[0], input_seq.shape[1], self.embed_size * 2])\n",
    "\n",
    "    return concat_embedding\n",
    "  \n",
    "  \n",
    "  def initialize_rnn(self, batch_size: int) -> torch.Tensor :\n",
    "    '''\n",
    "    This function returns initial hidden state for self.rnn for given batch_size\n",
    "    \n",
    "    Argument\n",
    "      batch_size (int): \n",
    "      \n",
    "    Return\n",
    "      initial_hidden_state (torch.Tensor):\n",
    "    '''\n",
    "    \n",
    "    return torch.zeros([self.num_layers, batch_size, self.hidden_size])\n",
    "  \n",
    "    \n",
    "  \n",
    "  def forward(self, input_seq:torch.LongTensor):\n",
    "    '''\n",
    "    Forward propgation of Melody Language Model.\n",
    "    \n",
    "    Argument\n",
    "      input_seq: A batch of melodies represented as a sequence of vector (pitch_idx, dur_idx). \n",
    "                 Has a shape of [num_batch, num_timesteps (num_notes), 2(pitch, dur)], or can be a PackedSequence\n",
    "                 벡터 (pitch_idx, dur_idx)의 시퀀스로 표현된 멜로디들의 집합으로 이루어진 배치. \n",
    "                 Shape은 [배치 샘플 수, 타임스텝의 수 (==음표의 수), 2 (음고, 길이)] 혹은 PackedSequence.\n",
    "    \n",
    "    Output\n",
    "      pitch_dist: Probability distribution of pitch of next upcoming note for each timestep 't'.\n",
    "                  Has a shape of [num_batch, numtimesteps, self.num_pitch]\n",
    "                매 타임 스텝 t에 대해, 그 다음에 등장할 음표 음고의 확률 분포\n",
    "      dur_dist: Probability distribution of duration of next upcoming note for each timestep 't'.\n",
    "                Has a shape of [num_batch, numtimesteps, self.num_dur]\n",
    "                매 타임 스텝 t에 대해, 그 다음에 등장할 음표 길이의 확률 분포\n",
    "      \n",
    "    '''\n",
    "      \n",
    "  \n",
    "    '''\n",
    "    TODO: Complete this function. You have to handle both cases: input_seq as ordinary Tensor / input_seq as PackedSequence\n",
    "    If the input_seq is PackedSequence, return PackedSequence\n",
    "    \n",
    "    \n",
    "    input_seq → self.get_concat_embedding → self.rnn → self.final_layer → torch.softmax for [pitch, duration]\n",
    "    \n",
    "    Follow the instruction\n",
    "    '''\n",
    "\n",
    "    if isinstance(input_seq, torch.Tensor): # If input is an ordinary tensor\n",
    "\n",
    "      # 1. Get concatenated_embeddings using self.get_concat_embedding\n",
    "      concatenated_embeddings = self.get_concat_embedding(input_seq)\n",
    "      \n",
    "      # 2. Put concatenated_embeddings to self.rnn.\n",
    "      # Remember: RNN, GRU, LSTM returns two outputs\n",
    "      output, _ = self.rnn(concatenated_embeddings)\n",
    "      \n",
    "      # 3. Put rnn's output with a shape of [num_batch, num_timestep, hidden_size] to self.final_layer\n",
    "      logits = self.final_layer(output)\n",
    "      \n",
    "      # 4. Convert logits (output of self.final_layer) to pitch probability and duration probability\n",
    "      # Caution! You have to get separately softmax-ed pitch and duration\n",
    "      # Because you have to pick one pitch and one duration from the probability distribution\n",
    "      pitch_dist = torch.softmax(logits[:,:,:self.num_pitch], dim=-1)\n",
    "      dur_dist = torch.softmax(logits[:,:,self.num_pitch:], dim=-1)\n",
    "\n",
    "      return pitch_dist, dur_dist\n",
    "\n",
    "    elif isinstance(input_seq, PackedSequence):      \n",
    "      # 1. Get concatenated_embeddings using self.get_concat_embedding\n",
    "      # To get concatenated_embeddings, You have to either pad_packed_sequence(input_seq, batch_first=True)\n",
    "      # Or use input_seq.data, and then make new PackedSequence using concatenated_embeddings as data, and copy batch_lengths, sorted_indices, unsorted_indices.\n",
    "      padded_sequence, batch_lengths = pad_packed_sequence(input_seq, batch_first=True)\n",
    "      concatenated_embeddings = self.get_concat_embedding(padded_sequence)\n",
    "      packed_sequence_of_concatenated = pack_padded_sequence(concatenated_embeddings, batch_lengths, enforce_sorted = False, batch_first=True)\n",
    "      \n",
    "      # 2. Put concatenated embedding to self.rnn\n",
    "      output, _ = self.rnn(packed_sequence_of_concatenated)\n",
    "      output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "      \n",
    "      # 3. Put rnn output to self.final_layer to get probability logit for pitch and duration\n",
    "      # Again, rnn's output is PackedSequence so you have to handle it\n",
    "      logits = self.final_layer(output_unpacked.data)\n",
    "      \n",
    "      # 4. Convert logits to pitch probability and duration probability\n",
    "      # Caution! You have to get separately softmax-ed pitch and duration\n",
    "      # Because you have to pick one pitch and one duration from the probability distribution\n",
    "      pitch_dist = torch.softmax(logits[:,:,:self.num_pitch], dim=-1)\n",
    "      dur_dist = torch.softmax(logits[:,:,self.num_pitch:], dim=-1)\n",
    "      \n",
    "      # Return output as PackedSequence\n",
    "      pitch_dist_packed = PackedSequence(pitch_dist, packed_sequence_of_concatenated.batch_sizes, packed_sequence_of_concatenated.sorted_indices, packed_sequence_of_concatenated.unsorted_indices)\n",
    "      dur_dist_packed = PackedSequence(dur_dist, packed_sequence_of_concatenated.batch_sizes, packed_sequence_of_concatenated.sorted_indices, packed_sequence_of_concatenated.unsorted_indices)\n",
    "\n",
    "      return pitch_dist_packed, dur_dist_packed\n",
    "    else:\n",
    "      print(f\"Unrecognized input type: {type(input_seq)}\")\n",
    "    \n",
    "    return\n",
    "  \n",
    "\n",
    "hidden_size = 64\n",
    "embed_size = 40\n",
    "    \n",
    "model = MelodyLanguageModel(hidden_size, embed_size, entire_set.get_vocabs())\n",
    "model(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "108a8117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your concart_embedding: \n",
      "tensor([[[ 2.1694,  0.6212,  0.3131,  ...,  0.4642,  1.3819,  0.8357],\n",
      "         [-1.3285, -1.3170, -0.0599,  ...,  0.5008, -0.1657, -0.0178],\n",
      "         [-0.1857,  0.3597, -0.8987,  ...,  0.5008, -0.1657, -0.0178],\n",
      "         ...,\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247],\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247],\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247]],\n",
      "\n",
      "        [[ 2.1694,  0.6212,  0.3131,  ...,  0.4642,  1.3819,  0.8357],\n",
      "         [-0.3346,  0.8694,  0.8537,  ...,  0.5008, -0.1657, -0.0178],\n",
      "         [-0.3346,  0.8694,  0.8537,  ...,  0.5008, -0.1657, -0.0178],\n",
      "         ...,\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247],\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247],\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247]],\n",
      "\n",
      "        [[ 2.1694,  0.6212,  0.3131,  ...,  0.4642,  1.3819,  0.8357],\n",
      "         [-0.4460, -0.9124,  0.7683,  ..., -0.0370, -1.5637, -0.3983],\n",
      "         [-0.7299, -0.0644,  2.2735,  ..., -0.0370, -1.5637, -0.3983],\n",
      "         ...,\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247],\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247],\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.1694,  0.6212,  0.3131,  ...,  0.4642,  1.3819,  0.8357],\n",
      "         [-0.4460, -0.9124,  0.7683,  ..., -0.0370, -1.5637, -0.3983],\n",
      "         [-0.3346,  0.8694,  0.8537,  ...,  0.5008, -0.1657, -0.0178],\n",
      "         ...,\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247],\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247],\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247]],\n",
      "\n",
      "        [[ 2.1694,  0.6212,  0.3131,  ...,  0.4642,  1.3819,  0.8357],\n",
      "         [ 1.7176,  0.5359, -0.3095,  ...,  0.1881, -2.1263,  0.5509],\n",
      "         [-0.4460, -0.9124,  0.7683,  ...,  0.1881, -2.1263,  0.5509],\n",
      "         ...,\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247],\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247],\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247]],\n",
      "\n",
      "        [[ 2.1694,  0.6212,  0.3131,  ...,  0.4642,  1.3819,  0.8357],\n",
      "         [-0.7299, -0.0644,  2.2735,  ...,  0.5008, -0.1657, -0.0178],\n",
      "         [-0.7299, -0.0644,  2.2735,  ...,  0.5008, -0.1657, -0.0178],\n",
      "         ...,\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247],\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247],\n",
      "         [-1.8102, -0.4222,  1.4417,  ...,  1.3259, -2.1149,  3.7247]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test model.get_concat_embedding\n",
    "'''\n",
    "batch = next(iter(train_loader))\n",
    "melody, shifted_melody = batch\n",
    "padded_melody, _ = pad_packed_sequence(melody, batch_first=True)\n",
    "\n",
    "concat_embedding = model.get_concat_embedding(padded_melody)\n",
    "print(f'Your concart_embedding: \\n{concat_embedding}')\n",
    "\n",
    "assert concat_embedding.shape[:-1] == padded_melody.shape[:-1], \"Num_batch and num_timestep of concat_embedding has to be the same with input melody\"\n",
    "assert concat_embedding.shape[2] == embed_size * 2, \"Error in size of embedding dimension\"\n",
    "assert (concat_embedding[0,0,:] == concat_embedding[1,0,:]).all(), \"Error: your embedding vectors for the same input notes are different\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0050c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test code with ordinary tensor (using batch_size=1)\n",
    "'''\n",
    "\n",
    "single_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "single_batch = next(iter(single_loader))\n",
    "single_melody, single_shifted_melody = single_batch\n",
    "pitch_out, dur_out = model(single_melody)\n",
    "\n",
    "assert pitch_out.shape == (1,single_melody.shape[1], model.num_pitch),  \\\n",
    "          f\"Error in pitch_out.shape. Expected {1,single_melody.shape[1], model.num_pitch}, but got {pitch_out.shape}\"\n",
    "assert dur_out.shape == (1,single_melody.shape[1], model.num_dur), \\\n",
    "          f\"Error in dur_out.shape. Expected {1,single_melody.shape[1], model.num_dur}, but got {dur_out.shape}\"\n",
    "\n",
    "assert (0<pitch_out).all() and (pitch_out<1).all() and (0<dur_out).all() and (dur_out<1).all(), \\\n",
    "          \"Every output must have a value between 0 and 1 \"\n",
    "assert (torch.abs(torch.sum(pitch_out, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every pitch class has to be 1\"\n",
    "assert (torch.abs(torch.sum(dur_out, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every duration class has to be 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test code with PackedSequence\n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, collate_fn=pack_collate, shuffle=True)\n",
    "batch = next(iter(train_loader))\n",
    "melody, shifted_melody = batch\n",
    "pitch_out, dur_out = model(melody)\n",
    "\n",
    "assert isinstance(pitch_out, type(melody)) and isinstance(dur_out, type(melody)), f\"Input of model was {type(melody)} but output is {type(pitch_out)}\"\n",
    "\n",
    "assert (pitch_out.batch_sizes == melody.batch_sizes).all(), \\\n",
    "          \"batch_sizes of input and output has to be the same\"\n",
    "assert len(pitch_out.data) == len(batch[0].data), \"Number of notes in input and output has to be the same\"\n",
    "assert (torch.abs(torch.sum(pitch_out.data, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every pitch class has to be 1\"\n",
    "assert (torch.abs(torch.sum(dur_out.data, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every duration class has to be 1\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734ec23",
   "metadata": {},
   "source": [
    "## Problem 6. Implement training loop (25 pts)\n",
    "- If you have succeeded in implementing model for PackedSequence, you can implement the training loop assuming that input batch is a PackedSequence\n",
    "- If not, you can implement the training loop using batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8d6c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_nll_loss(prob_distribution, correct_class):\n",
    "  '''\n",
    "  This function takes predicted probability distrubtion and the corresponding correct_class.\n",
    "  \n",
    "  For example,  prob_distribution = [[0.2287, 0.2227, 0.5487], [0.1301, 0.4690, 0.4010]] means that\n",
    "  for 0th data sample, the predicted probability for 0th category is 0.2287, for 1st category is 0.2227, and for 2nd category is 0.5487,\n",
    "  and for 1st data sample, the predicted probability for 0th category is 0.1301, for 1st category is 0.4690, and for 2nd category is 0.4010,\n",
    "  \n",
    "  Negative Log Likelihood, which is -y*log(y_hat), can be regarded as negative log value of predicted probability for correct class (y==1).\n",
    "  If the given correct_class is [1, 2], the loss for 0th data sample becomes negative log of [0.2287, 0.2227, 0.5487][1], which is -torch.log(0.2227), \n",
    "  because the correct category for this sample was 1st category, and the predicted probability was 0.2227\n",
    "  And the loss for 1st data sample becomes negative log of [0.1301, 0.4690, 0.4010][2], which is -torch.log(0.4010),\n",
    "  because the correct category for this sample was 2nd category, and the predicted probability was 0.4010\n",
    "  \n",
    "  To make implementation easy, let's assume we have 2D tensor for prob_distribution and  1D tensor for correct_class\n",
    "   \n",
    "  Arguments:\n",
    "    prob_distribution (2D Tensor)\n",
    "    correct_class (1D Tensor)\n",
    "    \n",
    "  Return:\n",
    "    loss (torch.Tensor): Negative log likelihood loss for every data sample in prob_distrubition. Has a same shape with correct_class\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  \n",
    "  Caution:  Do not return the mean loss. Return loss that has same shape with correct_class\n",
    "  Try not to use for loop, or torch.nn.CrossEntropyLoss, or torch.nn.NLLLoss\n",
    "  '''\n",
    "  assert prob_distribution.dim() == 2 and correct_class.dim() == 1, \"Let's assume we only take 2D tensor for prob_distribution and 1D tensor for correct_class\"\n",
    "  # Write your code from here\n",
    "  \n",
    "  return\n",
    "torch.manual_seed(0)\n",
    "prob_distribution = torch.softmax(torch.randn([10, 3]), dim=-1)\n",
    "correct_class = torch.randint(0,3, [10])\n",
    "print(f\"prob_distribution: \\n{prob_distribution}, \\n correct_class for each datasample: \\n {correct_class.unsqueeze(1)}\")\n",
    "\n",
    "loss = get_nll_loss(prob_distribution, correct_class)\n",
    "print('Loss: ', loss)\n",
    "assert (torch.abs(loss-torch.Tensor([1.5020, 0.7572, 0.4797, 0.7693, 0.4563, 0.8718, 0.7973, 1.3412, 1.6403, 0.2423]))<1e-4).all(), \"Error in loss value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963172fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_for_single_batch(model, batch, device):\n",
    "  '''\n",
    "  This function takes model and batch and calculate Cross Entropy Loss for given batch.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    batch (batch collated by pack_collate): Tuple of (melody_batch, shifted_melody_batch)\n",
    "    device (str): cuda or cpu. In which device to calculate the batch\n",
    "    \n",
    "  Return:\n",
    "    loss (torch.Tensor): Calculated mean loss for given model and batch. Has a shape of 0D tensor\n",
    "    \n",
    "  TODO: Complete this function using get_nll_loss().\n",
    "  Now you have to return the mean loss of every data sample in the batch \n",
    "  \n",
    "  Caution: You have to calculate loss for pitch, and loss for duration separately.\n",
    "  Then you can take average of pitch_loss and duration_loss\n",
    "  \n",
    "  Important Tip: If you are using PackedSequence, you can feed PackedSequence.data directly to get_nll_loss.\n",
    "  It makes the implementation much easier, because it doesn't need to reshape probabilty distribution and correct_class\n",
    "  '''\n",
    "  \n",
    "\n",
    "  return\n",
    "\n",
    "model.to('cuda')\n",
    "batch = next(iter(train_loader))\n",
    "out = get_loss_for_single_batch(model, batch, device='cuda')\n",
    "\n",
    "assert isinstance(out, torch.Tensor), \"Return value of get_loss_for_single_batch has to be torch.Tensor\"\n",
    "assert out.dim() == 0, \"Return value of get_loss_for_single_batch has to be torch.Tensor with dim 0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If you have implemented the previous function correctly, this code will train the model\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEV = 'cuda' # or cpu, but using cpu will be too slow\n",
    "model = MelodyLanguageModel(hidden_size, embed_size, entire_set.get_vocabs())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 30\n",
    "\n",
    "model.to(DEV)\n",
    "loss_record = []\n",
    "valid_loss_record = []\n",
    "best_valid_loss = torch.inf\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "  model.train()\n",
    "  for batch in tqdm(train_loader,leave=False):\n",
    "    loss = get_loss_for_single_batch(model, batch, device=DEV)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_record.append(loss.item())\n",
    "    \n",
    "  # Validation\n",
    "  with torch.inference_mode():\n",
    "    model.eval()\n",
    "    loss_for_entire_valid = 0\n",
    "    num_notes = 0\n",
    "    for batch in valid_loader:\n",
    "      loss = get_loss_for_single_batch(model, batch, device=DEV)\n",
    "      if isinstance(batch[0], PackedSequence):\n",
    "        n_note = len(batch[0].data)        \n",
    "      else:\n",
    "        n_note = batch[0].shape[1]\n",
    "        \n",
    "      loss_for_entire_valid += loss.item() * n_note\n",
    "      num_notes += n_note\n",
    "    valid_loss = loss_for_entire_valid/num_notes\n",
    "    if valid_loss < best_valid_loss:\n",
    "      best_valid_loss = valid_loss\n",
    "      torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "      torch.save(model.state_dict(), 'last_model.pt')\n",
    "    valid_loss_record.append(loss_for_entire_valid/num_notes)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be25b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('Training Loss')\n",
    "plt.plot(loss_record)\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Validation Loss')\n",
    "plt.plot(valid_loss_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e96ed2",
   "metadata": {},
   "source": [
    "## Problem 7: Implement Generation (25 pts)\n",
    "- In this problem, you have to generate a new melody using the trained model\n",
    "- Melody language model can generate a new sequence by sampling a new note for each timestep, and feed the generated new note again to the model to predict the next note\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd915d",
   "metadata": {},
   "source": [
    "### Problem 7-1: Implement model inference (15 pts)\n",
    "- Inference in Language model is little bit different from an ordniary forward loop during the training.\n",
    "    - While training, you have entire sequence, from beginning to end.\n",
    "    - During the inference, you have to generate one note, and then feed it as an input for the next step\n",
    "- You have to implement given functions one by one to complete `generate()`\n",
    "- In this problem, you can assume that the model is on cpu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can assume that model is on cpu during the Problem 7\n",
    "'''\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_input_and_hidden_state(model, batch_size=1):\n",
    "  '''\n",
    "  This function generates initial input vector and hidden state for model's GRU\n",
    "  \n",
    "  To generate a new sequence, you have to provide initial seed token, which is ['start', 'start'].\n",
    "  You have to make a initial vector that has [pitch_category_index_of_'start', duration_category_index_of_'start']\n",
    "  \n",
    "  You also have to initial hidden state for the model's RNN.\n",
    "  In uni-directional RNN(or GRU), hidden state of RNN has to be a zero tensor with shape of (num_layers, batch_size, hidden_size)\n",
    "\n",
    "  \n",
    "  Argument:\n",
    "    model (MelodyLanguageModel)\n",
    "    \n",
    "  Returns:\n",
    "    initial_input_vec (torch.Tensor): Has a shape of [batch_size, 1 (timestep), 2]\n",
    "    initial_hidden (torch.Tensor): Has a shape of [num_layers, bach_size, hidden_size]\n",
    "    \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  \n",
    "  return\n",
    "\n",
    "batch_size = 2\n",
    "input_vec, initial_hidden = get_initial_input_and_hidden_state(model, batch_size=batch_size)\n",
    "print(f'input_vec: \\n{input_vec} \\n initial_hidden: \\n {initial_hidden}')\n",
    "\n",
    "assert input_vec.ndim == 3\n",
    "assert initial_hidden.ndim == 3\n",
    "assert input_vec.shape == (batch_size, 1, 2)\n",
    "assert initial_hidden.shape == (model.num_layers, batch_size, model.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c5e91",
   "metadata": {},
   "source": [
    "### Hint: Sampling from distribution\n",
    "- The language model predict probability distribution of pitch and duration for  upcoming note\n",
    "- To do that, you have to know how to sample a result from a given probability distribution\n",
    "- In PyTorch, you can use `atensor.multinomial(num_samples)`\n",
    "    - In this assignment you don't have to sample more than 1, but \n",
    "    - multinomial(num_samples=100, replacement=False) means that you want to sample 100 samples without overlapping category\n",
    "        - Thus, the total class has to be larger than 100, because you cannot sample a single category multiple time\n",
    "    - multinomial(num_samples=100, replacement=True) means that you will sample 100 from the distrubtion independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23845568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "'''\n",
    "Example of sampling a result from a given probability distribution\n",
    "'''\n",
    "\n",
    "dummy_prob_distribution = torch.Tensor([0.1, 0.5, 0.2, 0.05, 0.15])\n",
    "sampled_out = dummy_prob_distribution.multinomial(num_samples=10000, replacement=True)\n",
    "print(sampled_out[:20])\n",
    "Counter(sampled_out.tolist()) # Number of each category sampled is almost same as num_samples * probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fef28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_single_step(model, cur_input, prev_hidden):\n",
    "  '''\n",
    "  This function runs MelodyLangaugeModel just for one step, for the given current input and previous hidden state.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    cur_input (torch.LongTensor): Input for the current time step. Has a shape of (batch_size=1, 1 (timestep), 2)\n",
    "    prev_hidden (torch.Tensor): Hidden state of RNN after previous timestep\n",
    "\n",
    "  Returns:\n",
    "    cur_output (torch.LongTensor): Sampled note [pitch_category_idx, duration_category_idx] from the predicted probability distribution, with shape of [1,1,2]\n",
    "    last_hidden (torch.Tensor): Hidden state of RNN\n",
    "  Think about running the model.forward() step-by-step.\n",
    "  \n",
    "  input_seq → self.get_concat_embedding → self.rnn → self.final_layer → torch.softmax for [pitch, duration] → sampled [pitch, duration]\n",
    "\n",
    "  TODO: Complete this function\n",
    "  Caution: You have to use torch.multinomial(replacement=False) to sample a note from the probability distribution.\n",
    "    You can also use replacement=True, but for the automatic evaulation, please use replacement=False.\n",
    "  '''\n",
    "  return \n",
    "\n",
    "input_vec, initial_hidden = get_initial_input_and_hidden_state(model, batch_size=1)\n",
    "out_note, last_hidden = predict_single_step(model, input_vec, initial_hidden)\n",
    "print(f'out_note: \\n{out_note} \\n last_hidden: \\n {last_hidden}')\n",
    "\n",
    "assert out_note.ndim == 3\n",
    "assert last_hidden.ndim == 3\n",
    "assert out_note.shape == (1,1,2)\n",
    "\n",
    "assert len(set([predict_single_step(model, input_vec, initial_hidden)[0] for i in range(5)]))==5, 'Generated output has to be different based on random sampling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_end_token(model, cur_output):\n",
    "  '''\n",
    "  During the generation, there is a possibility that the generated note predicted 'end' token for either pitch or duration.\n",
    "  (In fact, model can even estimate 'start' token during the generation even though it has very low probability)\n",
    "  \n",
    "  Using information among (model.pitch2idx, model.dur2idx, model.idx2pitch, model.idx2dur) to check whether the given cur_output has 'end' token or not.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    cur_output (torch.LongTensor): Assume it has shape of [1,1,2 (pitch_idx, duration_idx)]\n",
    "  \n",
    "  Return:\n",
    "    is_end_token (bool): True if cur_output include category index such as 'start' or 'end',\n",
    "                          else False.\n",
    "                          \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  \n",
    "  \n",
    "  return \n",
    "\n",
    "\n",
    "print(is_end_token(model, out_note))\n",
    "\n",
    "assert not is_end_token(model, torch.LongTensor([[[10, 7]]])), 'This is not end token'\n",
    "assert is_end_token(model, torch.LongTensor([[[2, 40]]])), 'This is end token'\n",
    "assert is_end_token(model, torch.LongTensor([[[25, 2]]])),  'This is end token'\n",
    "assert is_end_token(model, torch.LongTensor([[[2, 2]]])),  'This is end token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079ad9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "\n",
    "def generate(model, random_seed=0):\n",
    "  '''\n",
    "  This function generates a new melody sequence with a given model and random_seed.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    random_seed (int): Language model's inference will always generate different result, because it uses random sampling for the prediction.\n",
    "                       Therefore, if you want to reproduce the same generation result, you have to fix random_seed.\n",
    "  \n",
    "  Returns:\n",
    "    generated_note_sequence (torch.LongTensor): Has a shape of [num_generated_notes, 2]\n",
    "  \n",
    "  TODO: Complete this function using get_initial_input_and_hidden_state(), predict_single_step(), is_end_token()\n",
    "  \n",
    "  Hint: You can use while loop\n",
    "        You have to track the generated single note in a list or somewhere. \n",
    "  '''\n",
    "  \n",
    "  torch.manual_seed(random_seed) # To reproduce the result, we have to control random sequence\n",
    "  \n",
    "  '''\n",
    "  Write your code from here\n",
    "  '''\n",
    "\n",
    "  return\n",
    "gen_out = generate(model)\n",
    "print(f\"gen_out: \\n {gen_out}\")\n",
    "\n",
    "assert isinstance(gen_out, torch.LongTensor), f\"output of generate() has to be torch.LongTensor, not {type(gen_out)}\"\n",
    "assert gen_out.ndim == 2, f\"output of generate() has to be 2D tensor, not {gen_out.ndim}D tensor\"\n",
    "assert gen_out.shape[1] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be787bda",
   "metadata": {},
   "source": [
    "### Problem 7-2. Convert neural network's prediction to music score (10 pts)\n",
    "- Even though neural network has succeeded in generating a new sequence, it is just a sequence of index that neural network uses\n",
    "    - For example, generated note event [17, 10] means that this note has pitch value of 17th pitch category and duration value of 10th duration category\n",
    "- We have to convert categorical index to original value\n",
    "    - We saved this information as `idx2pitch`, `idx2dur` while we declared the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f18b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idx_pred_to_origin(pred:torch.Tensor, idx2pitch:list, idx2dur:list):\n",
    "  '''\n",
    "  This function convert neural net's output index to original pitch value (MIDI Pitch) and duration value \n",
    "  \n",
    "  Argument:\n",
    "    pred: generated output of the model. Has a shape of [num_notes, 2]. \n",
    "          0th dimension of each note represents pitch category index \n",
    "          and 1st dimension of each note represents duration category index\n",
    "  \n",
    "  Return:\n",
    "    converted_out (torch.Tensor): Has a same shape with 'pred'.\n",
    "    \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "    \n",
    "  return \n",
    "\n",
    "converted_out = convert_idx_pred_to_origin(gen_out, model.idx2pitch, model.idx2dur)\n",
    "assert converted_out.shape == gen_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be29831",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To solve the next problem, you have to know how note_representation looks like in muspy.\n",
    "\n",
    "In note representation, each note is represented as [start_timestep, pitch, duration, velocity]\n",
    "\n",
    "'''\n",
    "\n",
    "note_repr_example = train_set.dataset[0]\n",
    "note_repr_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067efdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_pitch_dur_to_note_representation(pitch_dur:torch.LongTensor):\n",
    "  '''\n",
    "  This function takes pitch_dur (shape of [num_notes, 2]) and returns the corresponding note representation (shape of [num_notes, 4])\n",
    "  In note representation, each note is represented as [start_timestep, pitch, duration, velocity]\n",
    "  \n",
    "  Since our generation is monophonic, you can regard start_timestep starts from 0 and accumulate the duration of note.\n",
    "  You can fix velocity to 64.\n",
    "  \n",
    "  \n",
    "  Arguments:\n",
    "    pitch_dur: LongTensor of note where each note represented as pitch and duration value\n",
    "    \n",
    "  return:\n",
    "    note_repr: numpy.Array with shape of [num_notes, 4]\n",
    "               each note has value of [start_timestep, pitch, duration, velocity]\n",
    "\n",
    "  TODO: Complete this function\n",
    "  Hint: You can use torch.cumsum() to accumulate the duration.\n",
    "  To convert torch tensor to numpy, you can use atensor.numpy()\n",
    "  \n",
    "  '''\n",
    "  \n",
    "  return \n",
    "\n",
    "note_repr = convert_pitch_dur_to_note_representation(converted_out)\n",
    "note_repr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "869368b0",
   "metadata": {},
   "source": [
    "# Submission\n",
    "- You have to copy and paste your code to ``MIR_Assignment_3.py`` file\n",
    "- Check that your code runs without error by running ``MIR_Assignment_3.py``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MIR_Assignment_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d9303",
   "metadata": {},
   "source": [
    "## Generation: Visualize and synthesize the generated result (10 pts)\n",
    "- Try to generate different melody using different `random_seed`\n",
    "- In your submission, include **Three** examples of your favorite among the generated results in wav\n",
    "    - You have to install soundfont and music font using \n",
    "        - `muspy.download_bravura_font()`\n",
    "        - `muspy.download_musescore_soundfont()`\n",
    "    - You may need fluidsynth to synthesize the sound.\n",
    "        - In colab, `!sudo apt-get install fluidsynth` will work\n",
    "        - In other Ubuntu os, `sudo apt-get update` and then `sudo apt-get install fluidsynth` will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71a3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "gen_music = muspy.from_note_representation(note_repr)\n",
    "gen_music.show_score()\n",
    "\n",
    "gen_audio = gen_music.synthesize().T\n",
    "ipd.Audio(gen_audio/2**15, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9a3cb",
   "metadata": {},
   "source": [
    "- Try with different random seed and generate interesting melodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_muspy_music(model, random_seed=0):\n",
    "  '''\n",
    "  This function combines 'generate', 'convert_idx_pred_to_origin', 'convert_pitch_dur_to_note_representation', muspy.from_note_representation\n",
    "  '''\n",
    "  gen_out = generate(model, random_seed)\n",
    "  converted_out = convert_idx_pred_to_origin(gen_out, model.idx2pitch, model.idx2dur)\n",
    "  note_repr = convert_pitch_dur_to_note_representation(converted_out)\n",
    "  gen_music = muspy.from_note_representation(note_repr)\n",
    "  return gen_music\n",
    "\n",
    "gen_music = generate_muspy_music(model, random_seed=2)\n",
    "gen_music.show_score()\n",
    "gen_audio = gen_music.synthesize().T\n",
    "ipd.Audio(gen_audio/2**15, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ddb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can save audio as wave file with muspy.write_audio\n",
    "'''\n",
    "gen_music.write_audio('result_0.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
